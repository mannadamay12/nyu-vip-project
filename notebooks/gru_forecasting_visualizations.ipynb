{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec49b6e",
   "metadata": {},
   "source": [
    "# GRU Forecasting & Visualization for Energy Markets\n",
    "\n",
    "This notebook trains the **GRU model** from the VIP project on both:\n",
    "\n",
    "1. **Sign classification task** (up/down direction)\n",
    "2. **Price regression task** (next-period price/return)\n",
    "\n",
    "and produces publication-ready visualizations suitable for the\n",
    "\"Survey of Machine Learning Methods for Energy Markets\" report.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "**This notebook uses real energy market data from the `Data/` folder.**\n",
    "\n",
    "The data pipeline (`data_pipeline.py`) reads from:\n",
    "- **Primary dataset**: `Data/Data_cleaned_Dataset.csv` - This is the main cleaned dataset containing electricity prices, volumes, natural gas prices, load data, temperature, and other engineered features.\n",
    "\n",
    "**Additional source files** (available in `Data/` but pre-processed into the main dataset):\n",
    "- `Net_generation_by places.csv`\n",
    "- `Net_generation_United_States_all_sectors_monthly.csv`\n",
    "- `Retail_sales_of_electricity_United_States_monthly.csv`\n",
    "\n",
    "The `load_dataset()` function in `data_pipeline.py` reads `Data_cleaned_Dataset.csv` and applies preprocessing (date parsing, interpolation, zero-price handling). The `make_dataset_for_task()` function then builds features and targets from this cleaned dataset.\n",
    "\n",
    "**All GRU training in this notebook uses the same unified data pipeline as the rest of the project**, ensuring consistency and reproducibility. No dummy or synthetic data is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71da0a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sequence length: 14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "# Ensure repo root is on the path (assumes notebook is in VIP/notebooks/)\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")\n",
    "\n",
    "import config\n",
    "from data_pipeline import make_dataset_for_task\n",
    "from models import model_gru\n",
    "from metrics import evaluate_model_outputs\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "print(\"Using sequence length:\", config.SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49c3e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Data Source Verification\n",
      "================================================================================\n",
      "[OK] Data_cleaned_Dataset.csv: EXISTS\n",
      "[OK] Net_generation_by places.csv: EXISTS\n",
      "[OK] Net_generation_United_States_all_sectors_monthly.csv: EXISTS\n",
      "[OK] Retail_sales_of_electricity_United_States_monthly.csv: EXISTS\n",
      "\n",
      "[OK] All CSV files found in Data/ folder\n",
      "\n",
      "================================================================================\n",
      "Main Dataset Inspection (Data_cleaned_Dataset.csv)\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (8034, 305)\n",
      "Date range: 2001-01-02 00:00:00 to 2022-12-31 00:00:00\n",
      "\n",
      "First few rows:\n",
      "  Trade Date  Electricity: Wtd Avg Price $/MWh  Electricity: Daily Volume MWh\n",
      "0 2001-01-02                            65.000                         1600.0\n",
      "1 2001-01-03                            61.250                         3200.0\n",
      "2 2001-01-04                            59.120                         4800.0\n",
      "3 2001-01-05                            59.215                         3800.0\n",
      "4 2001-01-06                            59.310                         2800.0\n",
      "\n",
      "Key columns present:\n",
      "  [OK] Trade Date\n",
      "  [OK] Electricity: Wtd Avg Price $/MWh\n",
      "  [OK] Electricity: Daily Volume MWh\n",
      "  [OK] Natural Gas: Henry Hub Natural Gas Spot Price (Dollars per Million Btu)\n",
      "  [OK] pjm_load sum in MW (daily)\n",
      "  [OK] temperature mean in C (daily): US\n",
      "\n",
      "[OK] Main dataset loaded successfully - using REAL data from CSV files\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gopalakrishnaabba/Downloads/VIP/notebooks/../data_pipeline.py:82: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.interpolate(method='linear', limit_direction='both')\n"
     ]
    }
   ],
   "source": [
    "# === Data Sanity Check: Verify CSV Files Exist ===\n",
    "\n",
    "# Verify all CSV files in Data/ folder exist\n",
    "print(\"=\" * 80)\n",
    "print(\"Data Source Verification\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "csv_files = [\n",
    "    \"Data_cleaned_Dataset.csv\",\n",
    "    \"Net_generation_by places.csv\",\n",
    "    \"Net_generation_United_States_all_sectors_monthly.csv\",\n",
    "    \"Retail_sales_of_electricity_United_States_monthly.csv\",\n",
    "]\n",
    "\n",
    "all_exist = True\n",
    "for fname in csv_files:\n",
    "    path = os.path.join(\"..\", \"Data\", fname)\n",
    "    exists = os.path.exists(path)\n",
    "    all_exist = all_exist and exists\n",
    "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
    "    print(f\"{status} {fname}: {'EXISTS' if exists else 'MISSING'}\")\n",
    "\n",
    "if not all_exist:\n",
    "    print(\"\\n[WARNING] Some CSV files are missing. The pipeline may fail.\")\n",
    "else:\n",
    "    print(\"\\n[OK] All CSV files found in Data/ folder\")\n",
    "\n",
    "# Load and inspect the main dataset used by the pipeline\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Main Dataset Inspection (Data_cleaned_Dataset.csv)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    from data_pipeline import load_dataset\n",
    "    \n",
    "    # Load a sample (first 1000 rows for quick inspection)\n",
    "    df_sample = load_dataset()\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df_sample.shape}\")\n",
    "    print(f\"Date range: {df_sample['Trade Date'].min()} to {df_sample['Trade Date'].max()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df_sample[['Trade Date', 'Electricity: Wtd Avg Price $/MWh', \n",
    "                     'Electricity: Daily Volume MWh']].head())\n",
    "    \n",
    "    print(f\"\\nKey columns present:\")\n",
    "    key_cols = [\n",
    "        'Trade Date',\n",
    "        'Electricity: Wtd Avg Price $/MWh',\n",
    "        'Electricity: Daily Volume MWh',\n",
    "        'Natural Gas: Henry Hub Natural Gas Spot Price (Dollars per Million Btu)',\n",
    "        'pjm_load sum in MW (daily)',\n",
    "        'temperature mean in C (daily): US'\n",
    "    ]\n",
    "    for col in key_cols:\n",
    "        present = \"[OK]\" if col in df_sample.columns else \"[MISSING]\"\n",
    "        print(f\"  {present} {col}\")\n",
    "    \n",
    "    print(\"\\n[OK] Main dataset loaded successfully - using REAL data from CSV files\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Error loading dataset: {e}\")\n",
    "    print(\"This may indicate a path issue. Check that Data/Data_cleaned_Dataset.csv exists.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221db0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training GRU for SIGN Classification\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Loading and preparing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gopalakrishnaabba/Downloads/VIP/notebooks/../data_pipeline.py:82: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df = df.interpolate(method='linear', limit_direction='both')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building features...\n",
      "Building targets for task: sign...\n",
      "Splitting data (time-based)...\n",
      "Train size: 5624, Val size: 1204, Test size: 1204\n",
      "Scaling features using standard scaler...\n",
      "Creating sequences with length 14...\n",
      "Sequence shapes - Train: (5610, 14, 20), Val: (1190, 14, 20), Test: (1190, 14, 20)\n",
      "Dataset preparation complete!\n",
      "[STEP 1] Data loading completed in 0.20 seconds\n",
      "\n",
      "Sign task shapes:\n",
      "  X_train: (5610, 14, 20)\n",
      "  X_val: (1190, 14, 20)\n",
      "  X_test: (1190, 14, 20)\n",
      "\n",
      "================================================================================\n",
      "[STEP 2] Starting GRU Training...\n",
      "================================================================================\n",
      "Configuration: max_epochs=100, batch_size=32, patience=10\n",
      "\n",
      "GRU Model Architecture:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_layer_1 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_layer_2 (\u001b[38;5;33mGRU\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m9,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_hidden (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,465</span> (103.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,465\u001b[0m (103.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,465</span> (103.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,465\u001b[0m (103.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Training GRU for TASK_TYPE = classification\n",
      "  max_epochs: 100\n",
      "  batch_size: 32\n",
      "  patience:   10\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 12:58:52.485797: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_15}}\n"
     ]
    }
   ],
   "source": [
    "# === 1. Train GRU for SIGN classification (direction) ===\n",
    "\n",
    "import sys\n",
    "\n",
    "# Set global task type for classification\n",
    "config.TASK_TYPE = \"classification\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Training GRU for SIGN Classification\")\n",
    "print(\"=\" * 80)\n",
    "sys.stdout.flush()\n",
    "\n",
    "try:\n",
    "    # Time the data loading\n",
    "    start_time = time.time()\n",
    "    print(\"\\n[STEP 1] Loading and preparing data...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    datasets_sign = make_dataset_for_task(\n",
    "        task_type=\"sign\",\n",
    "        seq_len=config.SEQUENCE_LENGTH,\n",
    "        test_size=config.TEST_SIZE,\n",
    "        val_size=config.VAL_SIZE,\n",
    "        scaler_type=config.SCALER_TYPE,\n",
    "    )\n",
    "    data_load_time = time.time() - start_time\n",
    "    print(f\"[STEP 1] Data loading completed in {data_load_time:.2f} seconds\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(\"\\nSign task shapes:\")\n",
    "    for k in [\"X_train\", \"X_val\", \"X_test\"]:\n",
    "        print(f\"  {k}:\", datasets_sign[k].shape)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Optional: smaller config for quicker experimentation; for full runs,\n",
    "    # comment this out to use config.GRU_CONFIG + global MAX_EPOCHS / BATCH_SIZE.\n",
    "    sign_train_config = {\n",
    "        **config.GRU_CONFIG,\n",
    "        \"max_epochs\": config.MAX_EPOCHS,\n",
    "        \"batch_size\": config.BATCH_SIZE,\n",
    "        \"patience\": config.EARLY_STOP_PATIENCE,\n",
    "    }\n",
    "\n",
    "    # Time the training\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"[STEP 2] Starting GRU Training...\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Configuration: max_epochs={sign_train_config['max_epochs']}, \"\n",
    "          f\"batch_size={sign_train_config['batch_size']}, \"\n",
    "          f\"patience={sign_train_config['patience']}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    training_start = time.time()\n",
    "\n",
    "    results_sign = model_gru.train_and_predict(datasets_sign, config=sign_train_config)\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"[STEP 2] Training completed!\")\n",
    "    print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    print(\"=\" * 80)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(\"\\n[STEP 3] Generating predictions and computing metrics...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    y_true_sign = datasets_sign[\"y_test\"]\n",
    "    y_pred_prob_sign = results_sign[\"y_pred_test\"].ravel()\n",
    "    y_pred_label_sign = (y_pred_prob_sign > 0.5).astype(int)\n",
    "\n",
    "    print(\"\\nSign classification summary:\")\n",
    "    unique, counts = np.unique(y_pred_label_sign, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    print(\"\\n[SUCCESS] All steps completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Training failed with error: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"\\nFull traceback:\")\n",
    "    traceback.print_exc()\n",
    "    sys.stdout.flush()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Train GRU for PRICE regression ===\n",
    "\n",
    "import sys\n",
    "\n",
    "# Set global task type for regression\n",
    "config.TASK_TYPE = \"regression\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Training GRU for PRICE Regression\")\n",
    "print(\"=\" * 80)\n",
    "sys.stdout.flush()\n",
    "\n",
    "try:\n",
    "    # Time the data loading\n",
    "    start_time = time.time()\n",
    "    print(\"\\n[STEP 1] Loading and preparing data...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    datasets_price = make_dataset_for_task(\n",
    "        task_type=\"price\",\n",
    "        seq_len=config.SEQUENCE_LENGTH,\n",
    "        test_size=config.TEST_SIZE,\n",
    "        val_size=config.VAL_SIZE,\n",
    "        scaler_type=config.SCALER_TYPE,\n",
    "    )\n",
    "    data_load_time = time.time() - start_time\n",
    "    print(f\"[STEP 1] Data loading completed in {data_load_time:.2f} seconds\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(\"\\nPrice task shapes:\")\n",
    "    for k in [\"X_train\", \"X_val\", \"X_test\"]:\n",
    "        print(f\"  {k}:\", datasets_price[k].shape)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    price_train_config = {\n",
    "        **config.GRU_CONFIG,\n",
    "        \"max_epochs\": config.MAX_EPOCHS,\n",
    "        \"batch_size\": config.BATCH_SIZE,\n",
    "        \"patience\": config.EARLY_STOP_PATIENCE,\n",
    "    }\n",
    "\n",
    "    # Time the training\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"[STEP 2] Starting GRU Training...\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Configuration: max_epochs={price_train_config['max_epochs']}, \"\n",
    "          f\"batch_size={price_train_config['batch_size']}, \"\n",
    "          f\"patience={price_train_config['patience']}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    training_start = time.time()\n",
    "\n",
    "    results_price = model_gru.train_and_predict(datasets_price, config=price_train_config)\n",
    "\n",
    "    training_time = time.time() - training_start\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"[STEP 2] Training completed!\")\n",
    "    print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    print(\"=\" * 80)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(\"\\n[STEP 3] Generating predictions and computing metrics...\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    y_true_price = datasets_price[\"y_test\"]\n",
    "    y_pred_price = results_price[\"y_pred_test\"].ravel()\n",
    "\n",
    "    print(\"\\nPrice regression basic metrics:\")\n",
    "    mse = mean_squared_error(y_true_price, y_pred_price)\n",
    "    mae = mean_absolute_error(y_true_price, y_pred_price)\n",
    "    r2 = r2_score(y_true_price, y_pred_price)\n",
    "    print(f\"MSE: {mse:.6f}, MAE: {mae:.6f}, R^2: {r2:.4f}\")\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    print(\"\\n[SUCCESS] All steps completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Training failed with error: {type(e).__name__}\")\n",
    "    print(f\"Error message: {str(e)}\")\n",
    "    import traceback\n",
    "    print(\"\\nFull traceback:\")\n",
    "    traceback.print_exc()\n",
    "    sys.stdout.flush()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Visualizations for PRICE regression ===\n",
    "\n",
    "time_index = np.arange(len(y_true_price))\n",
    "\n",
    "# (a) Time series: actual vs predicted\n",
    "plt.figure()\n",
    "plt.plot(time_index, y_true_price, label=\"Actual\", alpha=0.8)\n",
    "plt.plot(time_index, y_pred_price, label=\"Predicted (GRU)\", alpha=0.8)\n",
    "plt.xlabel(\"Test Time Index\")\n",
    "plt.ylabel(\"Price / Return (units)\")\n",
    "plt.title(\"GRU Price Regression: Actual vs Predicted (Test Set)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (b) Scatter plot with y=x line\n",
    "plt.figure()\n",
    "plt.scatter(y_true_price, y_pred_price, alpha=0.5)\n",
    "min_v = min(y_true_price.min(), y_pred_price.min())\n",
    "max_v = max(y_true_price.max(), y_pred_price.max())\n",
    "plt.plot([min_v, max_v], [min_v, max_v], linestyle=\"--\")\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"GRU Price Regression: True vs Predicted (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (c) Residual histogram\n",
    "residuals = y_true_price - y_pred_price\n",
    "plt.figure()\n",
    "plt.hist(residuals, bins=40, alpha=0.8)\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"GRU Price Regression: Residual Distribution (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (d) Rolling window RMSE to highlight regimes\n",
    "window = max(20, len(residuals) // 20)\n",
    "rolling_rmse = [\n",
    "    np.sqrt(np.mean(residuals[i:i+window] ** 2))\n",
    "    for i in range(0, len(residuals) - window + 1)\n",
    "]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(rolling_rmse)), rolling_rmse)\n",
    "plt.xlabel(\"Window index\")  # approximate time index\n",
    "plt.ylabel(f\"Rolling RMSE (window={window})\")\n",
    "plt.title(\"GRU Price Regression: Rolling RMSE (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Visualizations for SIGN classification ===\n",
    "\n",
    "# (a) Confusion matrix\n",
    "cm = confusion_matrix(y_true_sign, y_pred_label_sign)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(values_format=\"d\")\n",
    "plt.title(\"GRU Sign Classification: Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (b) ROC curve & AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true_sign, y_pred_prob_sign)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random baseline\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"GRU Sign Classification: ROC Curve (Test Set)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490da71",
   "metadata": {},
   "source": [
    "## Notes for the Survey Report\n",
    "\n",
    "- The **GRU architecture** here follows the standardized configuration in `config.GRU_CONFIG`\n",
    "  (two GRU layers + dense head) and the unified training utilities in `training_utils.py`.\n",
    "- Point forecast quality is summarized by MAE, MSE, RMSE, and $R^2$ for the price task.\n",
    "- Classification quality is summarized by accuracy, confusion matrix, and ROC/AUC.\n",
    "- Rolling RMSE illustrates **regime shifts**, which can be cross-referenced with crisis\n",
    "  periods discussed in the survey (e.g., COVID-19, 2022 energy shock).\n",
    "- These plots can be copied directly into the sections on RNN-based models and\n",
    "  evaluation protocols in the report.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
