{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRU Forecasting & Visualization for Energy Markets (PyTorch)\n",
        "\n",
        "This notebook trains the **GRU model** using **PyTorch** from the VIP project on both:\n",
        "\n",
        "1. **Sign classification task** (up/down direction)\n",
        "2. **Price regression task** (next-period price/return)\n",
        "\n",
        "and produces publication-ready visualizations suitable for the\n",
        "\"Survey of Machine Learning Methods for Energy Markets\" report.\n",
        "\n",
        "## Data Sources\n",
        "\n",
        "**This notebook uses real energy market data from the `Data/` folder.**\n",
        "\n",
        "The data pipeline (`data_pipeline.py`) reads from:\n",
        "- **Primary dataset**: `Data/Data_cleaned_Dataset.csv` - This is the main cleaned dataset containing electricity prices, volumes, natural gas prices, load data, temperature, and other engineered features.\n",
        "\n",
        "**Additional source files** (available in `Data/` but pre-processed into the main dataset):\n",
        "- `Net_generation_by places.csv`\n",
        "- `Net_generation_United_States_all_sectors_monthly.csv`\n",
        "- `Retail_sales_of_electricity_United_States_monthly.csv`\n",
        "\n",
        "The `load_dataset()` function in `data_pipeline.py` reads `Data_cleaned_Dataset.csv` and applies preprocessing (date parsing, interpolation, zero-price handling). The `make_dataset_for_task()` function then builds features and targets from this cleaned dataset.\n",
        "\n",
        "**All GRU training in this notebook uses the same unified data pipeline as the rest of the project**, ensuring consistency and reproducibility. No dummy or synthetic data is used.\n",
        "\n",
        "**Note:** This is the PyTorch implementation. For TensorFlow/Keras version, see `gru_forecasting_visualizations.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        ")\n",
        "\n",
        "# Ensure repo root is on the path (assumes notebook is in VIP/notebooks/)\n",
        "if \"..\" not in sys.path:\n",
        "    sys.path.append(\"..\")\n",
        "\n",
        "import config\n",
        "from data_pipeline import make_dataset_for_task\n",
        "from models.model_gru_pytorch import train_and_predict\n",
        "from metrics import evaluate_model_outputs\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
        "plt.rcParams[\"axes.grid\"] = True\n",
        "\n",
        "print(\"Using sequence length:\", config.SEQUENCE_LENGTH)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Data Sanity Check: Verify CSV Files Exist ===\n",
        "\n",
        "# Verify all CSV files in Data/ folder exist\n",
        "print(\"=\" * 80)\n",
        "print(\"Data Source Verification\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "csv_files = [\n",
        "    \"Data_cleaned_Dataset.csv\",\n",
        "    \"Net_generation_by places.csv\",\n",
        "    \"Net_generation_United_States_all_sectors_monthly.csv\",\n",
        "    \"Retail_sales_of_electricity_United_States_monthly.csv\",\n",
        "]\n",
        "\n",
        "all_exist = True\n",
        "for fname in csv_files:\n",
        "    path = os.path.join(\"..\", \"Data\", fname)\n",
        "    exists = os.path.exists(path)\n",
        "    all_exist = all_exist and exists\n",
        "    status = \"[OK]\" if exists else \"[MISSING]\"\n",
        "    print(f\"{status} {fname}: {'EXISTS' if exists else 'MISSING'}\")\n",
        "\n",
        "if not all_exist:\n",
        "    print(\"\\n[WARNING] Some CSV files are missing. The pipeline may fail.\")\n",
        "else:\n",
        "    print(\"\\n[OK] All CSV files found in Data/ folder\")\n",
        "\n",
        "# Load and inspect the main dataset used by the pipeline\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Main Dataset Inspection (Data_cleaned_Dataset.csv)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "try:\n",
        "    from data_pipeline import load_dataset\n",
        "    \n",
        "    df_sample = load_dataset()\n",
        "    \n",
        "    print(f\"\\nDataset shape: {df_sample.shape}\")\n",
        "    print(f\"Date range: {df_sample['Trade Date'].min()} to {df_sample['Trade Date'].max()}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    print(df_sample[['Trade Date', 'Electricity: Wtd Avg Price $/MWh', \n",
        "                     'Electricity: Daily Volume MWh']].head())\n",
        "    \n",
        "    print(f\"\\nKey columns present:\")\n",
        "    key_cols = [\n",
        "        'Trade Date',\n",
        "        'Electricity: Wtd Avg Price $/MWh',\n",
        "        'Electricity: Daily Volume MWh',\n",
        "        'Natural Gas: Henry Hub Natural Gas Spot Price (Dollars per Million Btu)',\n",
        "        'pjm_load sum in MW (daily)',\n",
        "        'temperature mean in C (daily): US'\n",
        "    ]\n",
        "    for col in key_cols:\n",
        "        present = \"[OK]\" if col in df_sample.columns else \"[MISSING]\"\n",
        "        print(f\"  {present} {col}\")\n",
        "    \n",
        "    print(\"\\n[OK] Main dataset loaded successfully - using REAL data from CSV files\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Error loading dataset: {e}\")\n",
        "    print(\"This may indicate a path issue. Check that Data/Data_cleaned_Dataset.csv exists.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1. Train GRU for SIGN classification (direction) ===\n",
        "\n",
        "import sys\n",
        "\n",
        "# Set global task type for classification\n",
        "config.TASK_TYPE = \"classification\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Training GRU (PyTorch) for SIGN Classification\")\n",
        "print(\"=\" * 80)\n",
        "sys.stdout.flush()\n",
        "\n",
        "try:\n",
        "    # Time the data loading\n",
        "    start_time = time.time()\n",
        "    print(\"\\n[STEP 1] Loading and preparing data...\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    datasets_sign = make_dataset_for_task(\n",
        "        task_type=\"sign\",\n",
        "        seq_len=config.SEQUENCE_LENGTH,\n",
        "        test_size=config.TEST_SIZE,\n",
        "        val_size=config.VAL_SIZE,\n",
        "        scaler_type=config.SCALER_TYPE,\n",
        "    )\n",
        "    data_load_time = time.time() - start_time\n",
        "    print(f\"[STEP 1] Data loading completed in {data_load_time:.2f} seconds\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    print(\"\\nSign task shapes:\")\n",
        "    for k in [\"X_train\", \"X_val\", \"X_test\"]:\n",
        "        print(f\"  {k}:\", datasets_sign[k].shape)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    sign_train_config = {\n",
        "        **config.GRU_CONFIG,\n",
        "        \"max_epochs\": config.MAX_EPOCHS,\n",
        "        \"batch_size\": config.BATCH_SIZE,\n",
        "        \"patience\": config.EARLY_STOP_PATIENCE,\n",
        "    }\n",
        "\n",
        "    # Time the training\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"[STEP 2] Starting GRU Training (PyTorch)...\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Configuration: max_epochs={sign_train_config['max_epochs']}, \"\n",
        "          f\"batch_size={sign_train_config['batch_size']}, \"\n",
        "          f\"patience={sign_train_config['patience']}\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    training_start = time.time()\n",
        "\n",
        "    results_sign = train_and_predict(datasets_sign, config=sign_train_config)\n",
        "\n",
        "    training_time = time.time() - training_start\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"[STEP 2] Training completed!\")\n",
        "    print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "    print(\"=\" * 80)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    print(\"\\n[STEP 3] Generating predictions and computing metrics...\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    y_true_sign = datasets_sign[\"y_test\"]\n",
        "    y_pred_prob_sign = results_sign[\"y_pred_test\"]\n",
        "    y_pred_label_sign = (y_pred_prob_sign > 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nSign classification summary:\")\n",
        "    unique, counts = np.unique(y_pred_label_sign, return_counts=True)\n",
        "    print(dict(zip(unique, counts)))\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"\\n[SUCCESS] All steps completed successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Training failed with error: {type(e).__name__}\")\n",
        "    print(f\"Error message: {str(e)}\")\n",
        "    import traceback\n",
        "    print(\"\\nFull traceback:\")\n",
        "    traceback.print_exc()\n",
        "    sys.stdout.flush()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2. Train GRU for PRICE regression ===\n",
        "\n",
        "import sys\n",
        "\n",
        "# Set global task type for regression\n",
        "config.TASK_TYPE = \"regression\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Training GRU (PyTorch) for PRICE Regression\")\n",
        "print(\"=\" * 80)\n",
        "sys.stdout.flush()\n",
        "\n",
        "try:\n",
        "    # Time the data loading\n",
        "    start_time = time.time()\n",
        "    print(\"\\n[STEP 1] Loading and preparing data...\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    datasets_price = make_dataset_for_task(\n",
        "        task_type=\"price\",\n",
        "        seq_len=config.SEQUENCE_LENGTH,\n",
        "        test_size=config.TEST_SIZE,\n",
        "        val_size=config.VAL_SIZE,\n",
        "        scaler_type=config.SCALER_TYPE,\n",
        "    )\n",
        "    data_load_time = time.time() - start_time\n",
        "    print(f\"[STEP 1] Data loading completed in {data_load_time:.2f} seconds\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    print(\"\\nPrice task shapes:\")\n",
        "    for k in [\"X_train\", \"X_val\", \"X_test\"]:\n",
        "        print(f\"  {k}:\", datasets_price[k].shape)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    price_train_config = {\n",
        "        **config.GRU_CONFIG,\n",
        "        \"max_epochs\": config.MAX_EPOCHS,\n",
        "        \"batch_size\": config.BATCH_SIZE,\n",
        "        \"patience\": config.EARLY_STOP_PATIENCE,\n",
        "    }\n",
        "\n",
        "    # Time the training\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"[STEP 2] Starting GRU Training (PyTorch)...\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Configuration: max_epochs={price_train_config['max_epochs']}, \"\n",
        "          f\"batch_size={price_train_config['batch_size']}, \"\n",
        "          f\"patience={price_train_config['patience']}\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    training_start = time.time()\n",
        "\n",
        "    results_price = train_and_predict(datasets_price, config=price_train_config)\n",
        "\n",
        "    training_time = time.time() - training_start\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"[STEP 2] Training completed!\")\n",
        "    print(f\"Total training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "    print(\"=\" * 80)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    print(\"\\n[STEP 3] Generating predictions and computing metrics...\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    y_true_price = datasets_price[\"y_test\"]\n",
        "    y_pred_price = results_price[\"y_pred_test\"]\n",
        "\n",
        "    print(\"\\nPrice regression basic metrics:\")\n",
        "    mse = mean_squared_error(y_true_price, y_pred_price)\n",
        "    mae = mean_absolute_error(y_true_price, y_pred_price)\n",
        "    r2 = r2_score(y_true_price, y_pred_price)\n",
        "    print(f\"MSE: {mse:.6f}, MAE: {mae:.6f}, R^2: {r2:.4f}\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    print(\"\\n[SUCCESS] All steps completed successfully!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Training failed with error: {type(e).__name__}\")\n",
        "    print(f\"Error message: {str(e)}\")\n",
        "    import traceback\n",
        "    print(\"\\nFull traceback:\")\n",
        "    traceback.print_exc()\n",
        "    sys.stdout.flush()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3. Visualizations for PRICE regression ===\n",
        "\n",
        "time_index = np.arange(len(y_true_price))\n",
        "\n",
        "# (a) Time series: actual vs predicted\n",
        "plt.figure()\n",
        "plt.plot(time_index, y_true_price, label=\"Actual\", alpha=0.8)\n",
        "plt.plot(time_index, y_pred_price, label=\"Predicted (GRU PyTorch)\", alpha=0.8)\n",
        "plt.xlabel(\"Test Time Index\")\n",
        "plt.ylabel(\"Price / Return (units)\")\n",
        "plt.title(\"GRU Price Regression (PyTorch): Actual vs Predicted (Test Set)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# (b) Scatter plot with y=x line\n",
        "plt.figure()\n",
        "plt.scatter(y_true_price, y_pred_price, alpha=0.5)\n",
        "min_v = min(y_true_price.min(), y_pred_price.min())\n",
        "max_v = max(y_true_price.max(), y_pred_price.max())\n",
        "plt.plot([min_v, max_v], [min_v, max_v], linestyle=\"--\")\n",
        "plt.xlabel(\"True Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"GRU Price Regression (PyTorch): True vs Predicted (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# (c) Residual histogram\n",
        "residuals = y_true_price - y_pred_price\n",
        "plt.figure()\n",
        "plt.hist(residuals, bins=40, alpha=0.8)\n",
        "plt.xlabel(\"Residual (True - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"GRU Price Regression (PyTorch): Residual Distribution (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# (d) Rolling window RMSE to highlight regimes\n",
        "window = max(20, len(residuals) // 20)\n",
        "rolling_rmse = [\n",
        "    np.sqrt(np.mean(residuals[i:i+window] ** 2))\n",
        "    for i in range(0, len(residuals) - window + 1)\n",
        "]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(len(rolling_rmse)), rolling_rmse)\n",
        "plt.xlabel(\"Window index\")\n",
        "plt.ylabel(f\"Rolling RMSE (window={window})\")\n",
        "plt.title(\"GRU Price Regression (PyTorch): Rolling RMSE (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4. Visualizations for SIGN classification ===\n",
        "\n",
        "# (a) Confusion matrix\n",
        "cm = confusion_matrix(y_true_sign, y_pred_label_sign)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(values_format=\"d\")\n",
        "plt.title(\"GRU Sign Classification (PyTorch): Confusion Matrix (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# (b) ROC curve & AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_true_sign, y_pred_prob_sign)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random baseline\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"GRU Sign Classification (PyTorch): ROC Curve (Test Set)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes for the Survey Report\n",
        "\n",
        "- The **GRU architecture** here follows the standardized configuration in `config.GRU_CONFIG`\n",
        "  (two GRU layers + dense head) implemented in PyTorch.\n",
        "- Point forecast quality is summarized by MAE, MSE, RMSE, and $R^2$ for the price task.\n",
        "- Classification quality is summarized by accuracy, confusion matrix, and ROC/AUC.\n",
        "- Rolling RMSE illustrates **regime shifts**, which can be cross-referenced with crisis\n",
        "  periods discussed in the survey (e.g., COVID-19, 2022 energy shock).\n",
        "- These plots can be copied directly into the sections on RNN-based models and\n",
        "  evaluation protocols in the report.\n",
        "- **PyTorch Implementation:** This notebook uses PyTorch instead of TensorFlow/Keras.\n",
        "  Results should be comparable to the TensorFlow version, allowing for framework comparison.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
